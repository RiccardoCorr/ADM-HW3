{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from aiohttp import ClientSession, ClientResponseError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Get the list of master's degree courses\n",
    "\n",
    "We start with the list of courses to include in your corpus of documents. In particular, we focus on web scrapping the MSc Degrees. Next, we want you to collect the URL associated with each site in the list from the previously collected list. The list is long and split into many pages. Therefore, we ask you to retrieve only the URLs of the places listed in the first 400 pages (each page has 15 courses, so you will end up with 6000 unique master's degree URLs).\n",
    "\n",
    "The output of this step is a .txt file whose single line corresponds to the master's URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's use BeautifulSoup and Requests to scrape the links related to universities present on the first 400 pages of the following website:\n",
    "'https://www.findamasters.com/masters-degrees/msc-degrees'.\n",
    "\n",
    "Note that it's possible to navigate to different pages by modifying the link in the final part, appending \"/?PG=\" + the page number.\n",
    "\n",
    "We are saving the URLs in each line of txt file called 'course_links'. In this case there aren't exceptions to handle with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a file .txt called course_links which contains in every line the URL of the link\n",
    "\n",
    "with open('course_links.txt', 'w') as file:\n",
    "    # scraping first page\n",
    "    response = requests.get('https://www.findamasters.com/masters-degrees/msc-degrees')\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    course_link = soup.find_all('a', class_='courseLink')\n",
    "    for link in course_link:\n",
    "        file.write(f\"www.findamasters.com{link.get('href')}\\n\") # writing URLs in the txt file\n",
    "        \n",
    "    # if response.status_code == 200:\n",
    "    for i in range(2, 401):\n",
    "        # scraping pages from 2 to 400\n",
    "        response = requests.get(f'https://www.findamasters.com/masters-degrees/msc-degrees/?PG={i}')\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        course_link = soup.find_all('a', class_='courseLink')\n",
    "        # writing URL's in the txt file\n",
    "        for link in course_link:\n",
    "            file.write(f\"www.findamasters.com{link.get('href')}\\n\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "        # adding a time.sleep of 1 second is important to avoid sending too many requests to the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Crawl master's degree pages\n",
    "Once you get all the URLs in the first 400 pages of the list, you:\n",
    "\n",
    "1. Download the HTML corresponding to each of the collected URLs.\n",
    "2. After you collect a single page, immediately save its HTML in a file. In this way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "3. Organize the downloaded HTML pages into folders. Each folder will contain the HTML of the courses on page 1, page 2, ... of the list of master's programs.\n",
    "\n",
    "**Tip**: Due to the large number of pages you should download, you can use some methods that can help you shorten the time. If you employed a particular process or approach, kindly describe it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The proposed solution for this task involves creating 400 folders, with each folder dedicated to a web page that was scraped in the previous exercise. Inside each of these folders, the corresponding HTML contents of the 15 website will be stored.\n",
    "\n",
    "The script is organized into three asynchronous functions:\n",
    "- *get_info(url, session, folder, page_number)*, which performs an asynchronous HTTP GET request to a line (URL) of the txt file created in the exercise above. It returns the html page, written in a specific folder. It manages exceptions such as \"Error 429: Too many requests to the website\" and if it happens there's a time.sleep of 1 second, until a new get request is sent.\n",
    "- *process_batch(urls_session, folder)*, which takes in input 15 urls and creates a list of asynchronous tasks, each corresponding to fetching HTML content from a URL in the given list using the *get_info* function, defined before. It uses asyncio.gather to concurrently execute all tasks and returns the results.\n",
    "- *main(urls, batch_size, starting_folder)* which creates the path where all the html are put. It iterates through batches of URLs, creating a sub-folder for each batch and calling process_batch to asynchronously download and save HTML content for each URL in the batch.\n",
    "\n",
    "\n",
    "The Python script is designed for asynchronous tasks using the aiohttp library to fetch HTML content from a list of URLs concurrently. \n",
    "\n",
    "In this particular case, working on multiple downloads at the same time are not so effective because the 6000 URLs are all from the same server, so we need to insert a time.sleep to handle \"Too many requests\". As a consequence, the code takes several hours to complete. To mitigate this, we introduced the *starting_folder* parameter in the main function. This allows us to resume the download process from where it left off, avoiding the need to recreate files and folders from scratch each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_info(url, session, folder, page_number):\n",
    "    try:\n",
    "        resp = await session.request(method=\"GET\", url=\"https://\" + url)\n",
    "        resp.raise_for_status()\n",
    "        html = await resp.text(encoding='utf-8')\n",
    "        \n",
    "        # Create the path in the corresponding folder\n",
    "        file_path = os.path.join(folder, f\"page_{int(page_number)}.html\")\n",
    "        \n",
    "        # Write the html page in the right folder\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(html)\n",
    "        \n",
    "        return html\n",
    "    \n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 429: # Error 429: too many requests.\n",
    "            print(f\"Received 429 error. Too many requests. Waiting for a while...\")\n",
    "            await asyncio.sleep(1)  # Wait for 1 second before retrying\n",
    "            return await get_info(url, session, folder, page_number)\n",
    "        else:\n",
    "            raise e  # Re-raise other ClientResponseError\n",
    "        \n",
    "async def process_batch(urls, session, folder):\n",
    "    tasks = [get_info(url, session, folder, page_number = count) for count, url in enumerate(urls, start=1)]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "async def main(urls, batch_size=15, starting_folder=0):\n",
    "    main_directory = \"master_programs_html\"\n",
    "    os.makedirs(main_directory, exist_ok=True)\n",
    "    \n",
    "    async with ClientSession() as session:\n",
    "        count_folder = starting_folder + 1\n",
    "        for i in range(starting_folder*batch_size, len(urls), batch_size):\n",
    "            # selecting the URLs from urls variable\n",
    "            batch_urls = urls[i:i + batch_size]\n",
    "            # Creating a sub-folder for every batch\n",
    "            folder_name = os.path.join(main_directory, f\"folder_{count_folder}\")\n",
    "            os.makedirs(folder_name, exist_ok=True)\n",
    "            count_folder += 1\n",
    "            # Downloading and writing file HTML in the batch\n",
    "            await process_batch(batch_urls, session, folder_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open('course_links.txt', 'r') as file:\n",
    "        urls = [line.strip() for line in file] # creating a list with the 6000 URLs from the lines of course_links.txt\n",
    "        \n",
    "    result = await main(urls, starting_folder = 305) # starting_folder represents the folder we are starting from\n",
    "\n",
    "    for text in result:\n",
    "        pass # text contains your html (text) response\n",
    "    print(\"Download and organization of HTML pages completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages\n",
    "At this point, you should have all the HTML documents about the master's degree of interest, and you can start to extract specific information. The list of the information we desire for each course and their format is as follows:\n",
    "\n",
    "- Course Name (to save as courseName): string;\n",
    "- University (to save as universityName): string;\n",
    "- Faculty (to save as facultyName): string\n",
    "- Full or Part Time (to save as isItFullTime): string;\n",
    "- Short Description (to save as description): string;\n",
    "- Start Date (to save as startDate): string;\n",
    "- Fees (to save as fees): string;\n",
    "- Modality (to save as modality):string;\n",
    "- Duration (to save as duration):string;\n",
    "- City (to save as city): string;\n",
    "- Country (to save as country): string;\n",
    "- Presence or online modality (to save as administration): string;\n",
    "- Link to the page (to save as url): string.\n",
    "\n",
    "For each master's degree, you create a course_i.tsv file of this structure:\n",
    "\n",
    "        courseName \\t universityName \\t  ... \\t url\n",
    "If an information is missing, you just leave it as an empty string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "First things first let's create the empty dataframe with the variables described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>courseName</th>\n",
       "      <th>universityName</th>\n",
       "      <th>facultyName</th>\n",
       "      <th>isItFullTime</th>\n",
       "      <th>description</th>\n",
       "      <th>startDate</th>\n",
       "      <th>fees</th>\n",
       "      <th>modality</th>\n",
       "      <th>duration</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>administration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [courseName, universityName, facultyName, isItFullTime, description, startDate, fees, modality, duration, city, country, administration]\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name dataframe columns\n",
    "\n",
    "columns = [\n",
    "    \"courseName\",\n",
    "    \"universityName\",\n",
    "    \"facultyName\",\n",
    "    \"isItFullTime\",\n",
    "    \"description\",\n",
    "    \"startDate\",\n",
    "    \"fees\",\n",
    "    \"modality\",\n",
    "    \"duration\",\n",
    "    \"city\",\n",
    "    \"country\",\n",
    "    \"administration\",\n",
    "]\n",
    "\n",
    "# Create a dataframe with the specific columns above\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Visualizza il DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to populate the dataframe, opening every html page from every folder created in the exercise 1.2.\n",
    "\n",
    "After this operation we have to find specific elements in the html page, handling exceptions, for example if there's no matching element with the *find_all* function, variable will be \"\".\n",
    "\n",
    "For every row of the dataframe we are also creating a .tsv file named as the master degree, containing the corresponding variables values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the dataframe\n",
    "path = \"master_programs_html\"\n",
    "os.mkdir(\"files_tsv\")\n",
    "count = 0\n",
    "for folder in range(1, 401): # loop for every folder\n",
    "    for file in range(1, 16): # loop for every file\n",
    "        file_path = os.path.join(path, f\"folder_{folder}\", f\"page_{file}.html\")\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='replace') as fl:\n",
    "            soup = BeautifulSoup(fl, 'html.parser')\n",
    "            courseName = soup.find(\"h1\", {\"class\": \"course-header__course-title\"}).get_text(strip = True)\n",
    "            universityName = soup.find(\"a\", {\"class\": \"course-header__institution\"}).get_text(strip = True)\n",
    "            facultyName = soup.find(\"a\", {\"class\": \"course-header__department\"}).get_text(strip = True)\n",
    "            extract = soup.find(\"span\", {\"class\": \"key-info__study-type\"})\n",
    "            if extract is None:\n",
    "                isItFullTime = \"\"\n",
    "            else:\n",
    "                isItFullTime = extract.get_text(strip = True)\n",
    "            description = soup.find(\"div\", {\"class\": \"course-sections__description\"}).find(\"div\", {\"class\": \"course-sections__content\"}).get_text(strip = True)\n",
    "            startDate = soup.find(\"span\", {\"class\": \"key-info__start-date\"}).get_text(strip = True)\n",
    "            # some entries do not have this field\n",
    "            extract = soup.find(\"div\", {\"class\": \"course-sections__fees\"})\n",
    "            if extract is None:\n",
    "                fees = \"\"\n",
    "            else:\n",
    "                fees = extract.find(\"div\", {\"class\": \"course-sections__content\"}).get_text(strip = True)\n",
    "            modality = soup.find(\"span\", {\"class\": \"key-info__qualification\"}).get_text(strip = True)\n",
    "            duration = soup.find(\"span\", {\"class\": \"key-info__duration\"}).get_text(strip = True)\n",
    "            city = soup.find(\"a\", {\"class\": \"course-data__city\"}).get_text(strip = True)\n",
    "            country = soup.find(\"a\", {\"class\": \"course-data__country\"}).get_text(strip = True)\n",
    "            extract1 = soup.find(\"a\", {\"class\": \"course-data__online\"})\n",
    "            extract2 = soup.find(\"a\", {\"class\": \"course-data__on-campus\"})\n",
    "            if extract1 is None and extract2 is None:\n",
    "                administration = \"\"\n",
    "            elif extract2 is None:\n",
    "                administration = extract1.get_text(strip = True)\n",
    "            elif extract1 is None:\n",
    "                administration = extract2.get_text(strip = True)\n",
    "            else:\n",
    "                administration = extract1.get_text(strip = True) + \" & \" + extract2.get_text(strip = True)\n",
    "\n",
    "            new_row_data = {\"courseName\": courseName,\n",
    "                            \"universityName\": universityName,\n",
    "                            \"facultyName\": facultyName,\n",
    "                            \"isItFullTime\": isItFullTime,\n",
    "                            \"description\": description,\n",
    "                            \"startDate\": startDate,\n",
    "                            \"fees\": fees,\n",
    "                            \"modality\": modality,\n",
    "                            \"duration\": duration,\n",
    "                            \"city\": city,\n",
    "                            \"country\": country,\n",
    "                            \"administration\": administration,\n",
    "                            }\n",
    "            df = pd.concat([df, pd.DataFrame([new_row_data])], ignore_index=True)\n",
    "            courseName = courseName.replace('|', '_').replace('&', '_').replace('<', '_').replace('>', '_').replace(',', '_').replace('-', '_').replace('.', '_').replace('\\'', '_').replace('/', '_')\n",
    "            if courseName[-1] == \" \":\n",
    "                courseName = courseName[:-1]\n",
    "            file_path = os.path.join('files_tsv', f'{courseName}.tsv')\n",
    "            pd.DataFrame([new_row_data]).to_csv(file_path, sep='\\t', index=False)\n",
    "            count += 1            \n",
    "            print(f\"Created {courseName}.tsv, {count}/6000\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
